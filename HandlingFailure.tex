\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Handling Failure In Distributed Systems\\
}

\author{\IEEEauthorblockN{Garrison Shepard, Shane Lester, Vincent Zheng}
\IEEEauthorblockA{\textit{Computer Science (Student)} \\
\textit{Hunter College, City University Of New York}\\
New York, United States Of America\\
garrison.shepard53@myhunter.cuny.edu}

}






\maketitle

\begin{abstract}
A universal problem in distributed computing is ensuring the system operates as intended in the event one or more components (computing entities) failing in the system. How failure is handled depends upon the architecture of the system and the services it offers. In general handling failures leads to an overall increase in the complexity of the system design and implementation. Building such a system can be a large investment of time, money and resources depending on the level of fault tolerance required. We observe the origins of this complexity and present a simple replication protocol “primary/backup” to reduce the design and implementation complexity of handling failure in a distributed system. However in our quest to achieve simplicity, some performance has been sacrificed and a simplifying assumption has been made the master server does not fail. More robust and as performant options for handling failures can and do exist, but lack simplicity, or require a special case. We compare and contrast our primary backup protocol against two different case studies where failures must be handled in a distributed environment; the Paxos Protocol and the MapReduce library. 
\end{abstract}

\begin{IEEEkeywords}
Distributed systems, fault tolerance, reliable computing, primary/backup, Paxos, mapreduce
\end{IEEEkeywords}

\section{Introduction}
To build a distributed system that can tolerate failure, the problem of building a reliable computing system out of unreliable components must be solved. Reliable meaning the the system can tolerate failure, and unreliable meaning components can and do fail. Observe the simplest case of this problem. Suppose a simple key-value service is built on top of a single component, offering two basic operations \(Get(key)\) and \(Put(key, value)\) and assume that components are unreliable. If the component hosting the key-value service fails, then the entire service fails. Suppose an additional component that is an exact replica of the key-value service is added to the system. Then if one of the components fails, the other is still available to run the service. The key-value service can now tolerate failure of up to one component. If the key-value service consists of \(N\) computing entities then it can tolerate \(N-1\) faults. Originally the service tolerated \(0\) faults. In general the mechanism chosen for handling failure will dictate how many faults the system can tolerate. 

The example above is not a complete solution to the problem of building a reliable computing system out of unreliable components. Consider the case where the two components that make up the key-value service \(E_1\) and \(E_2\) are both able to receive requests from clients. Assume that \(E_1\) and \(E_2\) update each other when values are added or changed by passing messages over the network to guarantee they remain exact replicas of each other after all transactions. Suppose a client \(C_1\) sends a request \(Put(1, 5)\) to \(E_1\) and immediately after, another client \(C_2\) sends a request \(Put(1,34)\) to \(E_2\). Say \(E_1\) finishes processing the request from \(C_1\)  before \(E_2\) finishes processing the request from \(C_2\) and \(E_1\) sends an update to \(E_2\). \(E_1\) now receives a \(Get(1)\) from \(C_1\) and \(E_1\) replies with a value of \(5\). Shortly after \(E_1\) processes the \(Get(1)\),\(E_2\) finishes processing its request and updates \(E_1\). Both \(E_1\) and \(E_2\) now contain \((1, 34)\). Notice they remain exact replicas of each other after the transaction. However If this key-value service consisted of a single component the result of Get(1) would have been \(34\) not \(5\) because a key-value service processes requests in the order that the requests are sent. Recall that the request \(Get(1)\) is sent last but retrieves the value of \(5\). Notice the key-value service behaves in a different manner than a single replica of the same service would. This is intolerable behavior. There must be a guarantee that transactions are processed in the correct order, otherwise clients data will not be saved properly. The correct order is the order in which a single replica of the service would process these transactions. In this case that is the order in which requests are sent. 

We will refer to this behavior as single replica semantics. By adding a second replica to the system the ordering problem mentioned above is introduced. To guarantee single replica semantics, the problem of correctly ordering requests must be solved. The mechanism for handling failure must guarantee single replica semantics when dealing with replicated mutable data. In our key-value service example above mutable data is being replicated.

\section{Naive Primary/Backup}
Guaranteeing single replica semantics can be  challenging and can quickly lead to a complex design and implementation of the system. Our primary/backup protocol provides a simple solution for guaranteeing single replica semantics for all replicas in the system. Here we make the necessary mental leaps to present our primary/back replication protocol by exploring a naive approach primary backup approach.

Observe the following scheme. A key value service with the same operations mentioned above is hosted on a system consisting of two exact replicas 
\(R_1\) and \(R_2\) and two clients \(C_1 \)and \(C_2\). Suppose replica \(R_1\) is designated as a leader (or primary) meaning \(R_1\) is the only replica that can receive requests and suppose \(R_2\) is a backup for \(R_1\) , meaning if \(R_1\) fails \(R_2\) can take over and become the new leader. Assume that before \( R_1\) allows a request to modify its state (i.e saved to \(R_1\)’s copy of the key-value service) the request will be forwarded to the backup \(R_2\). \(R_1\) waits for an acknowledgment from \(R_2\) before proceeding to process the request. This way \(R_1\) is certain that \(R_2\) received the value. This guarantees that both replicas will remain exact copies of each other. Now consider the earlier mentioned case (fig 1.1) where \(C_1\)  and \(C_2\) send requests \(Put(1, 5)\) and \(Put(1, 34)\) respectively and shortly after \(C_1\) sends a \(Get(1)\) request. Recall that \(R_1\) has been designated primary and therefore \(R_1\) is the only replica that can handle requests. So all three requests will be sent to \(R_1\). Since \(R_1\) is a single replica of the key-value service, requests will be processed in the order that a single replica of the key value service would process them in, which is precisely the order in which these requests were sent. Thus single replica semantics have been guaranteed. Notice \(Get(1)\) retrieves the value of \(34\) which matches the the previous example. 

One of the assumptions when building a reliable computing system is that components can fail. As mentioned above when the primary fails, the backup will take over. How do the components in this service detect failure? Assume that our replicas from the previous example \(R_1\) and \(R_2\) pass messages over the network to inform each other that they have not yet failed. More precisely \(R_1\) expects to receive a message from \(R_2\) in some bounded (finite) amount of time $\Delta$  where $\Delta$ is a constant defined the programmer and \(R_2\) and vice versa. If \(R_1\) does not receive a message from \(R_2\) in $\Delta$ amount of time then \(R_1\) will declare \(R_2\) dead and vice versa. 

Consider a case where the network does not always deliver messages in less than $\Delta$ time. Assume the following transactions; a client \(C_1\) sends \(Put(6, 89)\), a client \(C_2\) sends \(Put(6, 20)\), and shortly after \(C_1\) sends \(Get(1)\). Suppose before \(Put(6, 89)\)is processed by \(R_1\), \(R_1\) does not receive a message in $\Delta$ time from \(R_2\). \(R_1\) will declare \(R_2\) dead and then when \(Put(6, 89)\) arrives \(R_1\) will process this request. Say that \(R_2\) failed to send a message in $\Delta$ time because the network does not always deliver messages within $\Delta$ amount of time and \(R_2\) has not actually failed. Since \(R_1\) has declared \(R_2\) dead \(R_1\) is no longer sending messages to \(R_2\), therefore \(R_2\) will not receive a message from \(R_1\) in less than $\Delta$ time, thus \(R_2\) will declare \(R_1\) dead and be promoted to primary. Now both \(R_1\) and \(R_2\) are acting as primary and are no longer in sync this violates one of the basic assumptions of our scheme that only one replica can be primary and that replica handles all requests.  Now both replicas are handling requests. Say \(C_2\)’s request arrives to \(R_2\) shortly after it began acting as a primary. \(R_2\) will process this request and not send an update to \(R_1\) thus violating single replica semantics. As mentioned before, such behavior is intolerable. This presents a problem. The above scheme makes the assumption that networks are always synchronous, meaning that all messages sent across the network arrive within some bounded amount of time $\Delta$. However in practice networks are not always synchronous, meaning messages do not always arrive in a bounded amount of time $\Delta$. This wreaks havoc on the naive primary backup scheme. 

In practice networks are eventually synchronous meaning that usually messages arrive some in bounded amount of time $\Delta$, but sometimes they do not and the number of times they do not is bounded, however this bound is unknown. In other words, there is eventually a point in time where messages arrives in $\Delta$ amount of time. Thus this model is not sufficient in practice because of the behavior of the networks. 

\section{Primary/Backup}
Our primary/backup replication protocol makes an adjustments to the naive primary/backup replication scheme so that it can be used in practice. The architecture for our protocol requires at least the master component called the view service to run.  Also note the backup cannot exist without a primary. In the previous scheme primary and backup servers exchanged messages over the network to detect whether a component failed. In our protocol the view service is responsible for determining whether components are alive or dead and the view service handles assigning the roles of primary or backup to the components in the system.

The view service is added to account for the fact that the networks are not always synchronous in practice. In the previous scheme components would previously be declared dead if a check in message did not arrive in less than $\Delta$ amount of time. Our protocol has components \(S_1\) (primary) and \(S_2\) (backup) send messages to the view service \(V\) to notify the system that they have not failed. This works because it is assumed that \(V\) cannot fail. Thus there will never be a time where a \(S_1\) or \(S_2\) are erroneously declared one another dead because all check in messages are sent to the view service. Since a working system operates under the assumption that the view service cannot fail, there will eventually be a point in time where some message from a component arrives with in $\Delta$ time and thus successfully reaches \(V\). further there will never be a time where the \(V\) is erroneously declared dead thus, failure information will always be accurate. Recall that usually messages arrive within some bounded amount of time $\Delta$, but sometimes they do not and the number of times they do not is bounded, but this bound is not known. Suppose a case where \(V\) is waiting for a message from \(S_1\), and the bound on the number of times messages do not arrive ends up being \(100,000,000\) and $\Delta$ = 100ms. \(V\) should not wait \(27.7\) hours for a message from \(S_1\). In practice an estimated value for this unknown bound must be chosen. The estimated value chosen for a network should be tested on the network and can be found by trial and error. For our test bed the value of \(5\) was used as our estimated bound value, and messages where expected to arrive within $\Delta$ time where $\Delta$ = 100ms. This means if \(V\) did not receive a message from a component within 100ms five times in a row, the component would be declared dead. Thus \(V\) never waits longer than 500ms before declaring a server dead. 

Also note that our replication protocol can scaled to handle more faults. Components can keep being added to provide additional fault tolerance. In a system with N components where these components are not the view service, and after primary and backup are elected, N-2 servers are sending check-in messages to the view service to signal that they are ready and waiting to be elected. Our replication protocol can withstand N-1 faults (excluding view service). Our protocol ensures exact replicas in the same manner as the naive scheme. When a backup is first elected, the primary copies its entire database to the backup. From then on the primary sends updates to the backup before it processes requests to guarantee that the primary and backup remain exact replicas. Note that the copy and update operations apply exactly once semantics (at most once semantics with an unbounded amount of retries) when there is  backup in place.This ensures the backup is always properly replicated when some network messages fail to arrive. This scheme of designating one component to handle all requests automatically enforces the proper ordering of requests and thus guarantees single replica semantics for our replication protocol. 

However it is important to note that there are some shortcomings in our primary/backup replication protocol. The most notable issue is that it is assumed that the view service does not fail. However in practice it is possible for any component to fail. Assuming the system can experience some downtime this approach could still be applicable in practice. It offers substantial gains in reliability in comparison to a system without fault tolerance and is fairly straightforward to implement. Further safety mechanisms could be designed to ensure that components stop handling requests when the view service goes downs. However this has not been done in our test bed. Second our primary/backup replication is rather slow in comparison to other alternatives, it is particularly slow in cases where the amount of data being replicated is large and a new backup is elected. When a new backup is elected the primary must copy its entire database to the backup. If the database is large, this is an expensive operation. 

\section{Paxos}
A faster and more robust solution that solves the problem of building a reliable computing system out of unreliable components is the Paxos protocol. Paxos has no signal point of failure and therefore is a truly reliable approach. However, these gains in performance and reliability, come with a great deal of complexity in comparison to the primary backup scheme. 

The basic idea of Paxos is to guarantee single replica semantics by solving the ordering problem. Paxos solves the ordering problem via consensus. The idea that a majority of components must agree on a value before it is executed. 

Suppose two components \(S_1\) and \(S_2\) belong to the same system and assume the network is eventually synchronous. Suppose \(S_1\) and \(S_2\) go through a series of rounds and in each round one of these components is elected as a leader for the round via a round robin scheduling alogorithm. For simplicity assume components clocks are synchronized, meaning both components will advance to the next round to the next round at the same time. Suppose a round \(r\), where \(S_1\) is the leader attempts to send a message to \(S_2\) to decide on a value and a message does not arrive within $\Delta$ time. \(S_1\) suspects \(S_2\) of having failed and advances to round \(r+1\). In the next round \(S_2\) is now leader and attempts to send a message to \(S_1\). if \(S_2\) does receive a message in less than $\Delta$ time from \(S_1\), \(S_2\) will advance to round \(r+2\). This will go on until \(S_1\) and \(S_2\) eventually reach a round where they can communicate, that is a round where messages arrive within $\Delta$ time. This is guaranteed to happen because the network is eventually synchronous, however, we do not know the bound on the number of times messages will not be able to synchronous therefore we do not know how long this will take, further assuming components can crash, it is not possible to know in a given round \(r\) whether both components are still alive, if a \(S_1\) fails in a round r, then in a given round \(r + 1\), \(S_2\) will never hear from \(S_1\) and will transition to to a new round \(r+3\). in round \(r+3\) \(S_1\) is designated leader, however \(S_1\) failed, so \(S_2\) never receives a message and thus rounds keep advancing indefinitely. 

To account for this deficiency assume that only a minority of components in the system can fail and a third component \(S_3\) is present, that way in a given round \(r\) where messages take less than $\Delta$ to arrive , there will be always be  component available to respond to the message sent by the leader. Paxos operates in this round based manner, and in each round Paxos attempts to execute the same set of commands and is only successful in executing these commands at a point in time where the network is synchronous. To achieve consensus in a round a majority components must agree on a value. The current set of commands mentioned above when describing the round mechanism are not the actual commands Paxos executes in a given round. 

The steps Paxos takes in a given round r where messages take less than $\Delta$ are as follows. First a leader called the proposer  is elected in a round robin fashion. 

\subsection{Prepare Request}
Suppose \(S_1\) is elected as the proposer and \(S_2\) and \(S_3\) are acceptors, note that \(S_1\) is also an acceptor. \(S_1\) will send out a prepare request to all other components in this round \(S_2\) and \(S_3\) since in this round messages take less than delta and assuming \(S_2\) and \(S_3\) have not already crashed the prepare request will reach \(S_2\) and \(S_3\). This prepare request is asking what values they have seen so far. 

\subsection{Accept Request}
The next set of messages exchanged are called accept requests and these messages sent by the acceptors and are sent to the proposer, it is a response to the proposers prepare request. Each acceptor (including \(S_1\)) sends its initial value and round number that it has most recently accepted. Once these messages arrive to the proposer, the proposer runs a selection procedure that chooses the value with highest round number (no two messages will have have two unique values with the same round number, because only one value is decided upon per round, therefore if two values have the same round number they are the same value). 

\subsection{Proposal}
After the proposer selects a value, the proposer \(S_1\) sends out a proposal message to all acceptors \(S_1\) , \(S_2\), and \(S_3\). If an acceptor receives a proposal message it must accept the value (note whenever a value is sent a round number is sent along with it) that the proposal message contains. 

\subsection{Acceptance}
All acceptors in this round \(r\) send out an acceptance message to the proposer, once the proposer has received these acceptance messages from a majority of acceptors then a value has been chosen, consensus has been reached  the value can now be learned by all components. A value that is learned can be processed in a request to the service. 

Paxos is quite resilient and can handle failure at any point in a round. In many cases failures happen and there is still a majority of components that end accepting the value therefore nothing changes the consensus algorithm makes its decision because it has a majority. In cases where a decision cannot be reached or a proposer fails, an acceptor will take over the role as a proposer and start a new round and consensus will eventually be reached. 

It is important to note that in Paxos, components do not have synchronized clocks, meaning they execute on their own accord and pay no mind to the order in which other components are executing. In fact it is assumed that components are asynchronous. This means not all components will be in the same round at the same time meaning multiple proposers can exist in a single Paxos instance. This may sound alarming however it is not of much concern because if a proposer \(P_1\) attempts to send a message to a proposer \(P_2\) that is in a higher round \(P_2\) will reply to \(P_1\) informing \(P_1\) that it needs to catch up to the current round to communicate. The idea is their will be round where only one proposer exists and compoenents can communicate. It is also important to note that it is assumed that all components in a Paxos instance fail in a fail stop manner, meaning once a component has it is guaranteed that the component has failed and will no longer send messages in system. This is known as a non-byzantine fault. 

The assumptions that components are asynchronous, faults are of the non-byzantine variety and the network is eventually synchronous make up the asynchronous non-byzantine model. Which is exactly the model that the Paxos Protocol assumes. The approach Paxos takes to handling failure more complex than the primary backup replication protocol presented early. The process described above is solely to make a log of chosen requests on a single component. Every time a value is chosen, Paxos must go through this problem. To build a (key-value service as described earlier in the primary back-up replication protocol) would require each component to run an instance of the Paxos protocol. Each component would generate a log of values that have been chosen. From this set of distributed logs it would be up to developers of the system to assemble this distribute log into the proper ordering of requests to ensure single replica semantics. Further how this might be done is often implementation. Paxos is known for being complex to understand and implement.


\section{MapReduce}
Now let's examine the MapReduce implementation and how it handles failure of components in the system. MapReduce is a programming model with an associated implementation. The MapReduce library allows users to process large amounts of data in parallel across a distributed cluster without having to understand anything about parallelization or distributed systems. The user simply defines the map and reduce functions they want to use to perform their desired computations and the library will do the rest of the work.

Under the hood MapReduce architecture consists of many workers and a single master. The master is responsible for assigning jobs to workers and keeping track of which workers are alive and which workers are dead. The master is considered a single point of failure, but has relatively low chance of falling in comparison to its workers. Depending on the implementation there a different ways this failure is handled, for our testbed the computation is simply aborted. How map reduce works the data set is split into \(M\) pieces, which corresponds to the number of map jobs that must be completed.  The M map jobs return \(R\)  sets of intermediate key value pairs which corresponds to the R reduce jobs that must be performed. The important thing to note here is the output of a map job is the input of a reduce job, and the output of a reduce job is part of the result of the entire computation. Since a reduce job only depends on the result of a map job, all map operations do not have to complete before reduce job can begin. In fact only one map job has to complete for a map job to begin. However for simplicity in our test bed all map jobs were computed then all reduce jobs were computed. The input data that the Map Reduce job is replicated to the appropriate workers. 

MapReduce treats input data as immutable and this works well with map and reduce primitives, because these functions do not have side effects. A function without side effects is a function that for a given input \(x\) will always output \(y\). This is desirable because given the correct input, our function will always produce the correct output and if our input data is immutable (cannot be altered) then our input data will always be correct. Meaning any time we have some input data \(x\) for a map function \(m\), a map job can before formed that will produce the correct output \(y\). And any time there is some reduce function \(r\) and and  some correct output \(y\) from a map function, reduce job can be performed that will produce the correct output. Since jobs are loosely coupled and the data that is being operated on is immutable, the order in which requests are processed does not matter because the result of the computation cannot be adversely affected. The only rule that must be maintained is that reduce is fed correct output from some map job. 

This idea leads to a very simple approach to handling failure during a job or after one has completed. If a worker fails during or after a map job, all master has to do is simply execute the job. The job must re-executed if a worker failed in the middle of a map job because master does not know where the map job left off. In the event of a failure of a worker after a map job has already been completed the map reduce job must be executed because the result of a map job  is stored on the local buffer of a worker. In our test bed all output was stored in the global file system, but to stay true to the architecture our test bed re-executes map jobs on workers that fail. 

If a worker failed in the middle of a reduce job, the reduce job must be re-executed, however if the reduce job has already completed, the reduce job does not need to be re-executed because the output is stored in the global file system. The reason map reduce is able to provide such a solution to handling failure is, map reduce avoids replicating mutable data. All replicated data is immutable. Any data that mutable is isolated in one component the master component. Master keeps track of workers that are running and the jobs they are performing. Whenever mutable data is replicated, the correct order of operations that manipulate that data must be guaranteed. As seen earlier the solution to such ordering problems can become convoluted quickly. However, in many cases it is not possible to avoid the replication of mutable data, like in the case of the key-value service described earlier. 

\section{Conclusion}
The techniques Primary/Backup, Paxos, and MapReduce use to handle failure in a distributed environment each come with there own pros and cons. Primary/Backup offers a simple scheme for handling failure when replicated mutable data is present, but lacks performance and robustness needed for highly available system. The Paxos protocol offers performance and robustness needed for a highly available system but involves a very complex implementation. MapReduce offers the simplest scheme for handling failure but is a special case where all replicated data is immutable, which is often not the case. We have learned several things from this research, one handling failure in the presence of replicated mutable data, always results in the need to guarantee single replica semantics, which breaks down to solving the problem of guaranteeing that requests are processed in the correct order and this can be a challenging problem to solve with no single solution. We have also learned, when possible it is best to replicate immutable state, because handling failure in such a system can often be much easier. We have also learned that there is no cookie cutter solution to handling failure in distributed system. How failure is handled in a distributed system is strongly coupled to the services in which the system offers.

\end{document}
